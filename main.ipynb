{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V7fwoku01kp"
   },
   "source": [
    "# Parte 3: Preguntas y Respuestas en Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gEyGDebZ01kq"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para el preprocesamiento haremos las siguientes cosas:\n",
    "\n",
    "1. Pasar preguntas y contextos a arreglos de palabras.\n",
    "2. A partir campo `answer_start` crear un indice de palabra y no de caracter `answer_word_start`.\n",
    "3. Agregar campo `answer_word_end`\n",
    "\n",
    "Guardaremos el archivo en un JSON nuevo para no tener que repetir el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1530700333790,
     "user": {
      "displayName": "Freddie VENEGAS APABLAZA",
      "photoUrl": "//lh5.googleusercontent.com/-AV4JSEifrxo/AAAAAAAAAAI/AAAAAAAAAHg/mfr-4hyuy7Y/s50-c-k-no/photo.jpg",
      "userId": "109502576005160238836"
     },
     "user_tz": 240
    },
    "id": "J_jGr5tB01kr",
    "outputId": "db97b99a-0c9b-4ec7-f65e-75c787bde2d3"
   },
   "outputs": [],
   "source": [
    "# Importar dependencias para el preprocesamiento.\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 169661,
     "status": "error",
     "timestamp": 1530683460960,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "WqMZkRw_1WZT",
    "outputId": "2f54d8e4-791f-4c84-aedf-af7c4fca0f19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-ae6d230e-2956-495b-81ad-f13372c71402\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-ae6d230e-2956-495b-81ad-f13372c71402\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dev-v1.1-pr.json to dev-v1.1-pr.json\n",
      "Saving train-v1.1-pr.json to train-v1.1-pr.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-72edf3658aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     result = output.eval_js(\n\u001b[1;32m     69\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 70\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8JbiVq9301ku"
   },
   "outputs": [],
   "source": [
    "with open(\"train-v1.1.json\", \"r\") as data:\n",
    "    train = json.load(data)['data']\n",
    "with open(\"dev-v1.1.json\", \"r\") as data:\n",
    "    test = json.load(data)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3op8JuID01kw"
   },
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    for document in data:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            preprocess_paragraph(paragraph)\n",
    "\n",
    "def preprocess_paragraph(paragraph):\n",
    "    # preprocesamos contexto\n",
    "    preprocess_context(paragraph)\n",
    "    for question in paragraph['qas']:\n",
    "        # preprocesamos preguntas.\n",
    "        preprocess_question(paragraph['context'],question)\n",
    "\n",
    "def preprocess_context(paragraph):\n",
    "\n",
    "    # Guardamos contexto como arreglo preprocesado\n",
    "    paragraph['context_tokenized'] =  preprocess_text(paragraph['context'])\n",
    "\n",
    "def preprocess_question(context,question):\n",
    "    \n",
    "    # guardamos pregunta como arreglo\n",
    "    question['question_tokenized'] = preprocess_text(question['question'])\n",
    "    for answer in question['answers']:\n",
    "        # preprocesamos respuestas\n",
    "        preprocess_answer(context, answer)\n",
    "    \n",
    "def preprocess_answer(context,answer):\n",
    "    \n",
    "    # Pasamos respuesta a arreglo\n",
    "    answer['text_tokenized'] = preprocess_text(answer['text'])\n",
    "    \n",
    "\n",
    "    # Contamos cantidad de palabras hasta la respuesta\n",
    "    answer_word = len(preprocess_text(context[:answer['answer_start']]))\n",
    "    \n",
    "    # Guardamos en el hash\n",
    "    answer['answer_word_start'] = answer_word\n",
    "    \n",
    "    # Guardamos la palabra final de la respusta en el hash.\n",
    "    answer['answer_word_end'] = answer_word + len(answer['text_tokenized']) - 1\n",
    "    \n",
    "\"\"\"\n",
    "retorna string tokenizado y limpio de simbolos y mayusculas. Esto ultimo es necesario para disminuir\n",
    "las veces en que GLove no tiene la palabra.\n",
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    result = text\n",
    "    \n",
    "    # minusculas\n",
    "    result = result.lower()\n",
    "    \n",
    "    # simbolos para eliminar\n",
    "    symbols = re.sub(\"[{}]\".format(string.ascii_letters + \"'1234567890\" ),\"\",string.printable)\n",
    "    \n",
    "    # eliminamos simbolos mediante regexp.\n",
    "    result = re.sub(\"[{}–]\".format(symbols),\" \", result)\n",
    "    \n",
    "    return word_tokenize(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MWAMFcHP01kx"
   },
   "outputs": [],
   "source": [
    "# Preprocesamos y guardamos el nuevo dataset\n",
    "with open(\"train-v1.1-pr.json\",\"w\") as out:\n",
    "    print(\"preprocessing training set\")\n",
    "    preprocess_dataset(train)\n",
    "    json.dump(train,out)\n",
    "    \n",
    "with open(\"dev-v1.1-pr.json\",\"w\") as out:\n",
    "    print(\"preprocessing test set\")\n",
    "    preprocess_dataset(test)\n",
    "    json.dump(test,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vji4HUz01k0"
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "Para usar los embeddings e inyectarlos en el modelo de Keras primero se intento lo siguiente:\n",
    "\n",
    "1. Construiremos un index de palabras a partir del vocabulario del dataset.\n",
    "2. Transformamos las secuencias de palabras en secuencias de enteros mediante el index.\n",
    "3. Estandarizamos el Tamaño de la secuencia usando `pad_sequences` de Keras.\n",
    "4. Luego  construimos matriz de pesos a partir de Glove para inyectar a una capa `Embedding` de Keras.\n",
    "\n",
    "Este proceso no funcionó porque los vectores de glove ocupaban demasiada memoria de la gpu (eran 50M de parametros), a cambio se optó por precomputar los vectores de cada palabra y pasar los tensores de una sequencia directamente a la red. Como el dataset crece demasiado al pasar cada palabra a un vector de 300D, esta transformación se genera de a poco mediante un objeto Keras Sequence ( se probó primero un generador pero no era compatible con el paralelismo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "error",
     "timestamp": 1530743793087,
     "user": {
      "displayName": "Freddie VENEGAS APABLAZA",
      "photoUrl": "//lh5.googleusercontent.com/-AV4JSEifrxo/AAAAAAAAAAI/AAAAAAAAAHg/mfr-4hyuy7Y/s50-c-k-no/photo.jpg",
      "userId": "109502576005160238836"
     },
     "user_tz": 240
    },
    "id": "YYo2G52D01k0",
    "outputId": "a3bc0dca-70cc-4d2a-9580-356a04f341e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importamos dependencias para generar los embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import Sequence\n",
    "import threading\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "70i-0Uo201k4",
    "outputId": "76d852cc-a32c-461b-b64c-aecc9a2304d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running script\n",
      "Loading Glove Vectors\n"
     ]
    }
   ],
   "source": [
    "# Usamos los glove vectors 300D de wikipedia 2014. Por limitación de memoria no podemos usar un corpus más grande.\n",
    "glove_file = datapath(os.getcwd() + '/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(os.getcwd() + \"/test_word2vec.txt\")\n",
    "print(\"Running script\")\n",
    "#glove2word2vec(glove_file, tmp_file)\n",
    "print(\"Loading Glove Vectors\")\n",
    "embedder = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6kcj1IyU01k7"
   },
   "outputs": [],
   "source": [
    "# Funciones para metodo Embedding layer (se desecho)\n",
    "# retorna secuencia de enteros a partir de secuencia de palabras.\n",
    "def text_to_sequence(text_seq, word_index):\n",
    "    result = []\n",
    "    for word in text_seq:\n",
    "        if word in word_index:\n",
    "            result.append(word_index[word])\n",
    "        else:\n",
    "            word_index[word] = len(word_index)\n",
    "            result.append(word_index[word])\n",
    "    return np.array(result)\n",
    "\n",
    "# Construimos index de palabras a medida que aparecen en el dataset. \n",
    "# Construimos matrices de enteros a partir del dataset.\n",
    "def gen_data(dataset,word_index):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    output = []\n",
    "    for document in dataset:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                # Tomamos la primera respuesta para generar el dataset final\n",
    "                answer = question['answers'][0]\n",
    "                # Pasamos secuencias a secuencias de enteros.\n",
    "                contexts.append(text_to_sequence(paragraph['context_tokenized'],word_index))\n",
    "                questions.append(text_to_sequence(question['question_tokenized'],word_index))\n",
    "                #guardamos tupla de inicio y fin para el output.\n",
    "                output.append((answer['answer_word_start'],answer['answer_word_end']))\n",
    "    return contexts,questions,output,word_index\n",
    "\n",
    "\n",
    "\n",
    "# Funciones para la keras sequence.\n",
    "\n",
    "# Pasamos una secuencia de texto a un tensor de tamaño fijo.\n",
    "# Quedan en 0 el padding y las palabras que no estan en GLove\n",
    "def text_to_tensor(text_seq, word_vectors,sequence_length):\n",
    "    result = np.zeros((sequence_length,300))\n",
    "    for i,t in enumerate(text_seq):\n",
    "        if t in word_vectors:\n",
    "            result[i]  = word_vectors[t]\n",
    "    return result\n",
    "\n",
    "def data_counter(dataset):\n",
    "    count = 0\n",
    "    for document in dataset:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                count +=1\n",
    "    return count\n",
    "\n",
    "# Secuencia para hacer multijob data generation\n",
    "class TensorSequence(Sequence):\n",
    "\n",
    "    def __init__(self, dataset,batch_size,word_vectors,context_length,question_length):\n",
    "        self.dataset = []\n",
    "        self.batch_size = batch_size\n",
    "        self.data_count = data_counter(dataset)\n",
    "        self.word_vectors = word_vectors\n",
    "        self.context_length = context_length\n",
    "        self.question_length = question_length\n",
    "        print(\"Loading sequence\")\n",
    "        # Guardamos el dataset en formato tabla para poder indexar por batch size.\n",
    "        for document in dataset:\n",
    "            for paragraph in document['paragraphs']:\n",
    "                for question in paragraph['qas']:\n",
    "                    # Tomamos la primera respuesta para generar el dataset final\n",
    "                    answer = question['answers'][0]\n",
    "                    \n",
    "                    context = paragraph['context_tokenized'] if len(paragraph['context_tokenized']) <= context_length else paragraph['context_tokenized'][:context_length]\n",
    "                    question_t = question['question_tokenized'] if len(question['question_tokenized']) <= question_length else question['question_tokenized'][:question_length]\n",
    "                    self.dataset.append([context,question_t,min(answer['answer_word_start'],context_length-1),min(answer['answer_word_end'],context_length-1)])\n",
    "\n",
    "    # steps per batch\n",
    "    def __len__(self):\n",
    "        return self.data_count//self.batch_size\n",
    "\n",
    "    \n",
    "    \n",
    "    # retorna un batch de tensores.\n",
    "    def __getitem__(self, idx):\n",
    "        contexts = None\n",
    "        questions = None\n",
    "        output_start = []\n",
    "        output_end = []\n",
    "        #iteramos sobre el batch pedido.\n",
    "        for row in self.dataset[idx * self.batch_size:(idx + 1) * self.batch_size]:\n",
    "            \n",
    "            # pasamos sequencias de palabras a tensores\n",
    "            if contexts is None:\n",
    "                contexts = text_to_tensor(row[0],self.word_vectors,self.context_length)\n",
    "            else:\n",
    "                contexts = np.dstack((contexts,text_to_tensor(row[0],self.word_vectors,self.context_length)))\n",
    "            if  questions is None:\n",
    "                questions = text_to_tensor(row[1],self.word_vectors,self.question_length)\n",
    "            else:\n",
    "                questions= np.dstack((questions,text_to_tensor(row[1],self.word_vectors,self.question_length)))\n",
    "\n",
    "            #guardamos tupla de inicio y fin para el output.\n",
    "            output_start.append(row[2])\n",
    "            output_end.append(row[3])\n",
    "        #print(\"RETURNING a batch\")\n",
    "        return [np.moveaxis(contexts,2,0),np.moveaxis(questions,2,0)],[to_categorical(np.array(output_start),num_classes=self.context_length),to_categorical(np.array(output_end),num_classes=self.context_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tbIDyewh01k9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# generamos las secuencias de enteros para inyectar en el modelo de Keras.\n",
    "with open(\"train-v1.1-pr.json\", \"r\") as data:\n",
    "    train = json.load(data)\n",
    "with open(\"dev-v1.1-pr.json\", \"r\") as data:\n",
    "    test = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "acZa4q8b01k_"
   },
   "outputs": [],
   "source": [
    "# Calculamos el tamaño máximo\n",
    "def max_context(document):\n",
    "    return max(document['paragraphs'], key= lambda x: len(x['context_tokenized']))\n",
    "\n",
    "def max_question_par(paragraph):\n",
    "    return max(paragraph['qas'], key= lambda x: len(x['question_tokenized']))\n",
    "\n",
    "def max_paragraph(document):\n",
    "    return max(document['paragraphs'], key=lambda x: len(max_question_par(x)['question_tokenized']))\n",
    "\n",
    "MAX_CONTEXT = 400\n",
    "MAX_QUESTIONS = 30\n",
    "TRAIN_COUNT = data_counter(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGyb7XXC01lC"
   },
   "source": [
    "## Declaración del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9446,
     "status": "ok",
     "timestamp": 1530753505291,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "SFJ2r9Yi01lC",
    "outputId": "af01e5ff-73c8-4575-8069-dea03e2e6ceb"
   },
   "outputs": [],
   "source": [
    "# Importamos dependencias\n",
    "from keras.layers import Input, Concatenate, Dense, Reshape, Activation,Multiply, Dot, Add, Lambda,SeparableConv1D, BatchNormalization,TimeDistributed,Dropout,Reshape,Softmax, Reshape, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras.backend as K\n",
    "import keras.initializers\n",
    "import numpy as np\n",
    "\n",
    "# Para elegir GPU o multicore\n",
    "num_cores = 4\n",
    "CPU= False\n",
    "GPU= not CPU\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "if CPU:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sYQSmEEZ7MgY"
   },
   "outputs": [],
   "source": [
    "## Attention \n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output\n",
    "\n",
    "#https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout):\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        self.qs_layers = []\n",
    "        self.ks_layers = []\n",
    "        self.vs_layers = []\n",
    "        for _ in range(n_head):\n",
    "            self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "            self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "            self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        #self.layer_norm = BatchNormalization(axis=1)\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "        heads = []\n",
    "        #attns = []\n",
    "        for i in range(n_head):\n",
    "            qs = self.qs_layers[i](q)   \n",
    "            ks = self.ks_layers[i](k) \n",
    "            vs = self.vs_layers[i](v) \n",
    "        #head, attn = self.attention(qs, ks, vs, mask)\n",
    "            head = self.attention(qs, ks, vs, mask)\n",
    "            heads.append(head)\n",
    "        #attns.append(attn)\n",
    "        head = Concatenate()(heads)\n",
    "        #attn = Concatenate()(attns)\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        outputs = Add()([outputs, q])\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class EncoderConv():\n",
    "  \n",
    "    def __init__(self,n_convs,filters,kernel,name=\"encoder_conv\"):\n",
    "        self.n_convs = n_convs\n",
    "        self.filters = filters\n",
    "        self.kernel = kernel\n",
    "        self.name = name\n",
    "        self.norms = []\n",
    "        self.convs = []\n",
    "        for i in range(self.n_convs):\n",
    "            norm_layer = BatchNormalization(axis = 1,name=name+\"_norm_{}\".format(i))\n",
    "            conv_layer = SeparableConv1D(filters=filters,kernel_size=kernel,name=name+\"_conv_{}\".format(i),padding=\"same\")\n",
    "            self.norms.append(norm_layer)\n",
    "            self.convs.append(conv_layer)\n",
    "\n",
    "\n",
    "    def __call__(self,value):\n",
    "        value_normed = self.norms[0](value)\n",
    "        value_conv = self.convs[0](value_normed)\n",
    "        value_end = value_conv\n",
    "        for i in range(1,self.n_convs):\n",
    "            value_normed = value_normed = self.norms[i](value_end)\n",
    "            value_conv = self.convs[i](value_normed)\n",
    "            value_end = Add()([value_conv,value_end])\n",
    "\n",
    "        return value_end\n",
    "\n",
    "class SelfAttention():\n",
    "  \n",
    "    def __init__(self,n_heads,d_model,d_k,d_v,dropout=0.1,name=\"encoder_self_attention\"):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        self.name = name\n",
    "        self.attn = MultiHeadAttention(n_heads,d_model,d_k,d_v,dropout)\n",
    "        self.norm_layer = BatchNormalization(axis = 1,name=name+\"_norm\")\n",
    "        #self.mask = Lambda(lambda x:GetPadMask(x,x))(norm_layer)\n",
    "        \n",
    "    def __call__(self,value):\n",
    "        norm_layer = self.norm_layer(value)\n",
    "        attn_layer = self.attn(norm_layer,norm_layer,norm_layer)\n",
    "        value = Add()([value,attn_layer])\n",
    "        return value\n",
    "      \n",
    "class FeedForward():\n",
    "  \n",
    "    def __init__(self,ndims,activation=\"relu\",name=\"encoder_ff\"):\n",
    "        self.ndims = ndims\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        self.norm_layer = BatchNormalization(axis=1,name=name+\"_norm\")\n",
    "        self.ff = Dense(ndims, activation=activation, name=name+\"_ff\")\n",
    "      \n",
    "    def __call__(self,value):\n",
    "        norm = self.norm_layer(value)\n",
    "        ff = self.ff(norm)\n",
    "        value = Add()([value,ff])\n",
    "        return value\n",
    "\n",
    "      \n",
    "class EncoderBlock():\n",
    "  \n",
    "    ''' \n",
    "      Ensamble de Encoder Block\n",
    "      Para las Stacked Embedding EB, n_conv = 4\n",
    "      Para las Stacked Model EB, n_conv = 2 (Luego necesito repetir el EB 7 veces y tener 3 repeticiones de eso con pesos compartidos)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_convs, filters, kernels, n_heads, d_model, d_k, d_v, ndims, dropout=0.1, activation=\"relu\", name=\"encoder_block\"):\n",
    "        self.name = name\n",
    "        self.dropout = dropout\n",
    "        self.encoder_conv = EncoderConv(n_convs, filters, kernels, name=name+\"_conv\")\n",
    "        self.self_attention = SelfAttention(n_heads, d_model, d_k, d_v, dropout,name=name+\"_self_attention\")\n",
    "        self.ff = FeedForward(ndims, activation,name=name+\"_ff\")\n",
    "\n",
    "    def __call__(self, value):\n",
    "        enc_conv = self.encoder_conv(value)\n",
    "        self_att = self.self_attention(enc_conv)\n",
    "        value = self.ff(self_att)\n",
    "        return value\n",
    "\n",
    "class ModelEncoder():\n",
    "  \n",
    "    '''\n",
    "     Concatenación de n_reps Enconder blocks\n",
    "    '''\n",
    "    def __init__(self, n_reps, n_convs, filters, kernels, n_heads, d_model, d_k, d_v, ndims, dropout=0.1, activation=\"relu\", name=\"model_encoder\"):\n",
    "        self.blocks = []\n",
    "        self.n_reps = n_reps\n",
    "        self.name = name\n",
    "        for i in range(self.n_reps):\n",
    "            self.blocks.append(EncoderBlock(n_convs, filters, kernels, n_heads, d_model, d_k, d_v, ndims, dropout,name=name+\"_block_{}\".format(i)))\n",
    "\n",
    "    def __call__(self, value):\n",
    "        for i in range(self.n_reps):\n",
    "            value = self.blocks[i](value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1530753506875,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "w0aV_cmE01lE",
    "outputId": "d22bd9b1-e551-477f-db38-c7652fcf57bd"
   },
   "outputs": [],
   "source": [
    "## Highway network https://gist.github.com/iskandr/a874e4cf358697037d14a17020304535\n",
    "def highway_layers(value, n_layers, activation=\"tanh\", gate_bias=-3,name=\"highway\"):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gate_bias_initializer = keras.initializers.Constant(gate_bias)\n",
    "    for i in range(n_layers):     \n",
    "        gate = Dense(units=dim, bias_initializer=gate_bias_initializer,name=name+\"_dense_1_{}\".format(i))(value)\n",
    "        gate = Activation(\"sigmoid\",name=name+\"_activation_1_{}\".format(i))(gate)\n",
    "        negated_gate = Lambda(\n",
    "            lambda x: 1.0 - x,\n",
    "            output_shape=(dim,))(gate)\n",
    "        transformed = Dense(units=dim,name=name+\"_dense_2_{}\".format(i))(value)\n",
    "        transformed = Activation(activation,name=name+\"_activation_2_{}\".format(i))(value)\n",
    "        transformed_gated = Multiply(name=name+\"_multiply_1_{}\".format(i))([gate, transformed])\n",
    "        identity_gated = Multiply(name=name+\"_multiply_2_{}\".format(i))([negated_gate, value])\n",
    "        value = Add(name=name+\"_add_{}\".format(i))([transformed_gated, identity_gated])\n",
    "    return value\n",
    "  \n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "  \n",
    "def create_mask(x):\n",
    "    zeros = K.zeros_like(x)\n",
    "    return K.cast(K.not_equal(zeros,x), dtype='float32')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1530753508703,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "U8xFOuOHn4y5",
    "outputId": "4bea630e-9ef0-4a09-a669-22ab2b336f2d"
   },
   "outputs": [],
   "source": [
    "def attention(batch):\n",
    "  \n",
    "    def _attention_f(c_q):\n",
    "        c,q=c_q[:MAX_CONTEXT,:], c_q[MAX_CONTEXT:,:]\n",
    "        c = K.tile(c,[MAX_QUESTIONS,1])\n",
    "        q = K.reshape(K.tile(q,[1,MAX_CONTEXT]),[MAX_QUESTIONS*MAX_CONTEXT,FILTERS])\n",
    "        return K.concatenate([q,c,c*q],axis=1)\n",
    "\n",
    "    return K.map_fn(_attention_f,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6jXV7sRO01lG"
   },
   "outputs": [],
   "source": [
    "## Model params\n",
    "GLOVE_DIM=300\n",
    "KERNEL_SIZE=7\n",
    "FILTERS=64\n",
    "BLOCK_CONV_LAYERS=4\n",
    "N_HEADS=4\n",
    "DROPOUT=0.1\n",
    "N_REPS = 3\n",
    "BLOCK_CONV_LAYERS_STACKED = 2\n",
    "STACKED_KERNEL_SIZE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rbnC4hqk01lK"
   },
   "outputs": [],
   "source": [
    "## Question embedding\n",
    "question_input = Input(shape=(MAX_QUESTIONS,GLOVE_DIM),name=\"question_input\")\n",
    "highway_question = highway_layers(question_input,2,activation=\"relu\", gate_bias=-3,name=\"question_highway\")\n",
    "question_ff = EncoderBlock(BLOCK_CONV_LAYERS,FILTERS,KERNEL_SIZE,N_HEADS,FILTERS,FILTERS,FILTERS,FILTERS,DROPOUT,name=\"question_eeb\")(highway_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lRV44vJJ01lO"
   },
   "outputs": [],
   "source": [
    "## context embedding\n",
    "context_input = Input(shape=(MAX_CONTEXT,GLOVE_DIM),name=\"context_input\")\n",
    "highway_context = highway_layers(context_input,2,activation=\"relu\", gate_bias=-3,name=\"context_highway\")\n",
    "context_ff = EncoderBlock(BLOCK_CONV_LAYERS,FILTERS,KERNEL_SIZE,N_HEADS,FILTERS,FILTERS,FILTERS,FILTERS,DROPOUT,name=\"context_eeb\")(highway_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sKyJKeh301lR"
   },
   "outputs": [],
   "source": [
    "## Context question attention\n",
    "concat = Concatenate(axis=1)([context_ff,question_ff])\n",
    "lambda_concat = Lambda(attention)(concat)\n",
    "attention_dense = TimeDistributed(Dense(1,use_bias=False))(lambda_concat)\n",
    "attention_matrix = Reshape((MAX_CONTEXT,MAX_QUESTIONS))(attention_dense)\n",
    "attention_matrix_bar = Softmax()(attention_matrix)\n",
    "A = Dot(axes=(2,1))([attention_matrix_bar, question_ff])\n",
    "\n",
    "attention_matrix_transpose = Lambda(lambda x : K.permute_dimensions(x, (0, 2, 1)))(attention_matrix)\n",
    "attention_matrix_bar_bar = Softmax()(attention_matrix_transpose)\n",
    "B = Dot(axes=(2,1))([attention_matrix_bar, attention_matrix_bar_bar])\n",
    "B = Dot(axes=(2,1))([B, context_ff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6423,
     "status": "ok",
     "timestamp": 1530757704553,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "nPOjQOTv2zQe",
    "outputId": "842d813e-ea80-498b-d914-ed77d3809542"
   },
   "outputs": [],
   "source": [
    "## Stacked model encoder blocks.\n",
    "A_attention = Multiply()([context_ff,A])\n",
    "B_attention = Multiply()([context_ff,B])\n",
    "\n",
    "stacked_blocks_input=Concatenate(axis=2)([context_ff,A,A_attention,B_attention])\n",
    "\n",
    "stacked_blocks_resized = SeparableConv1D(filters=FILTERS,kernel_size=STACKED_KERNEL_SIZE,name=\"conv_resize\",padding=\"same\")(stacked_blocks_input)\n",
    "\n",
    "\n",
    "me = ModelEncoder(N_REPS, BLOCK_CONV_LAYERS_STACKED,FILTERS,STACKED_KERNEL_SIZE,N_HEADS,FILTERS,FILTERS,FILTERS,FILTERS,DROPOUT)\n",
    "\n",
    "stacked_encoder_blocks_0 = me(stacked_blocks_resized)\n",
    "stacked_encoder_blocks_1 = me(stacked_encoder_blocks_0)\n",
    "stacked_encoder_blocks_2 = me(stacked_encoder_blocks_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OIhFvGF3mof1"
   },
   "outputs": [],
   "source": [
    "## Output layer\n",
    "\n",
    "start_layer = Concatenate(axis=2)([stacked_encoder_blocks_0,stacked_encoder_blocks_1]) # no estoy seguro del axis\n",
    "start_dense = TimeDistributed(Dense(1,use_bias=False))(start_layer)\n",
    "start_reshape = Flatten()(start_dense)\n",
    "start_output = Softmax()(start_reshape)\n",
    "\n",
    "\n",
    "end_layer = Concatenate(axis=2)([stacked_encoder_blocks_0,stacked_encoder_blocks_2]) # no estoy seguro del axis\n",
    "end_dense = TimeDistributed(Dense(1, use_bias=False))(end_layer)\n",
    "end_reshape = Flatten()(end_dense)\n",
    "end_output = Softmax()(end_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 16941
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1530757706343,
     "user": {
      "displayName": "ANDRES CADIZ VIDAL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109343968497772879483"
     },
     "user_tz": 240
    },
    "id": "iIUEdf04br_7",
    "outputId": "0a466373-3a80-442c-dc95-77a24e852d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, 400, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 30, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_dense_1_0 (Dens (None, 400, 300)     90300       context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_dense_1_0 (Den (None, 30, 300)      90300       question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_activation_1_0  (None, 400, 300)     0           context_highway_dense_1_0[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_activation_1_0 (None, 30, 300)      0           question_highway_dense_1_0[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_activation_2_0  (None, 400, 300)     0           context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 300)          0           context_highway_activation_1_0[0]\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_activation_2_0 (None, 30, 300)      0           question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 300)          0           question_highway_activation_1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "context_highway_multiply_1_0 (M (None, 400, 300)     0           context_highway_activation_1_0[0]\n",
      "                                                                 context_highway_activation_2_0[0]\n",
      "__________________________________________________________________________________________________\n",
      "context_highway_multiply_2_0 (M (None, 400, 300)     0           lambda_11[0][0]                  \n",
      "                                                                 context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_multiply_1_0 ( (None, 30, 300)      0           question_highway_activation_1_0[0\n",
      "                                                                 question_highway_activation_2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_multiply_2_0 ( (None, 30, 300)      0           lambda_1[0][0]                   \n",
      "                                                                 question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_add_0 (Add)     (None, 400, 300)     0           context_highway_multiply_1_0[0][0\n",
      "                                                                 context_highway_multiply_2_0[0][0\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_add_0 (Add)    (None, 30, 300)      0           question_highway_multiply_1_0[0][\n",
      "                                                                 question_highway_multiply_2_0[0][\n",
      "__________________________________________________________________________________________________\n",
      "context_highway_dense_1_1 (Dens (None, 400, 300)     90300       context_highway_add_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_dense_1_1 (Den (None, 30, 300)      90300       question_highway_add_0[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_activation_1_1  (None, 400, 300)     0           context_highway_dense_1_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_activation_1_1 (None, 30, 300)      0           question_highway_dense_1_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_activation_2_1  (None, 400, 300)     0           context_highway_add_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 300)          0           context_highway_activation_1_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_activation_2_1 (None, 30, 300)      0           question_highway_add_0[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           question_highway_activation_1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "context_highway_multiply_1_1 (M (None, 400, 300)     0           context_highway_activation_1_1[0]\n",
      "                                                                 context_highway_activation_2_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "context_highway_multiply_2_1 (M (None, 400, 300)     0           lambda_12[0][0]                  \n",
      "                                                                 context_highway_add_0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "question_highway_multiply_1_1 ( (None, 30, 300)      0           question_highway_activation_1_1[0\n",
      "                                                                 question_highway_activation_2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_multiply_2_1 ( (None, 30, 300)      0           lambda_2[0][0]                   \n",
      "                                                                 question_highway_add_0[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "context_highway_add_1 (Add)     (None, 400, 300)     0           context_highway_multiply_1_1[0][0\n",
      "                                                                 context_highway_multiply_2_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "question_highway_add_1 (Add)    (None, 30, 300)      0           question_highway_multiply_1_1[0][\n",
      "                                                                 question_highway_multiply_2_1[0][\n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_norm_0 (BatchN (None, 400, 300)     1600        context_highway_add_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_norm_0 (Batch (None, 30, 300)      120         question_highway_add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_conv_0 (Separa (None, 400, 64)      21364       context_eeb_conv_norm_0[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_conv_0 (Separ (None, 30, 64)       21364       question_eeb_conv_norm_0[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_norm_1 (BatchN (None, 400, 64)      1600        context_eeb_conv_conv_0[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_norm_1 (Batch (None, 30, 64)       120         question_eeb_conv_conv_0[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_conv_1 (Separa (None, 400, 64)      4608        context_eeb_conv_norm_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_conv_1 (Separ (None, 30, 64)       4608        question_eeb_conv_norm_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 400, 64)      0           context_eeb_conv_conv_1[0][0]    \n",
      "                                                                 context_eeb_conv_conv_0[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 30, 64)       0           question_eeb_conv_conv_1[0][0]   \n",
      "                                                                 question_eeb_conv_conv_0[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_norm_2 (BatchN (None, 400, 64)      1600        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_norm_2 (Batch (None, 30, 64)       120         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_conv_2 (Separa (None, 400, 64)      4608        context_eeb_conv_norm_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_conv_2 (Separ (None, 30, 64)       4608        question_eeb_conv_norm_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 400, 64)      0           context_eeb_conv_conv_2[0][0]    \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 30, 64)       0           question_eeb_conv_conv_2[0][0]   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_norm_3 (BatchN (None, 400, 64)      1600        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_norm_3 (Batch (None, 30, 64)       120         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_conv_conv_3 (Separa (None, 400, 64)      4608        context_eeb_conv_norm_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_conv_conv_3 (Separ (None, 30, 64)       4608        question_eeb_conv_norm_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 400, 64)      0           context_eeb_conv_conv_3[0][0]    \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 30, 64)       0           question_eeb_conv_conv_3[0][0]   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_self_attention_norm (None, 400, 64)      1600        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_self_attention_nor (None, 30, 64)       120         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_21 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_23 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_24 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 400, 400)     0           time_distributed_14[0][0]        \n",
      "                                                                 time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 400, 400)     0           time_distributed_17[0][0]        \n",
      "                                                                 time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 400, 400)     0           time_distributed_20[0][0]        \n",
      "                                                                 time_distributed_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 400, 400)     0           time_distributed_23[0][0]        \n",
      "                                                                 time_distributed_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 30, 30)       0           time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 30, 30)       0           time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 30, 30)       0           time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 30, 30)       0           time_distributed_10[0][0]        \n",
      "                                                                 time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 400, 400)     0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 400, 400)     0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 400, 400)     0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 400, 400)     0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 30, 30)       0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 30, 30)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 30, 30)       0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 30, 30)       0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 400, 400)     0           activation_5[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_25 (TimeDistri (None, 400, 64)      4096        context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 30)       0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 30, 64)       4096        question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 400, 64)      0           dropout_3[0][0]                  \n",
      "                                                                 time_distributed_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 400, 64)      0           dropout_3[1][0]                  \n",
      "                                                                 time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 400, 64)      0           dropout_3[2][0]                  \n",
      "                                                                 time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 400, 64)      0           dropout_3[3][0]                  \n",
      "                                                                 time_distributed_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 30, 64)       0           dropout_1[0][0]                  \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 30, 64)       0           dropout_1[1][0]                  \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 30, 64)       0           dropout_1[2][0]                  \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 30, 64)       0           dropout_1[3][0]                  \n",
      "                                                                 time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 400, 256)     0           lambda_14[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 256)      0           lambda_4[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_26 (TimeDistri (None, 400, 64)      16448       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 30, 64)       16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 400, 64)      0           time_distributed_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30, 64)       0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 400, 64)      0           dropout_4[0][0]                  \n",
      "                                                                 context_eeb_self_attention_norm[0\n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 30, 64)       0           dropout_2[0][0]                  \n",
      "                                                                 question_eeb_self_attention_norm[\n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 400, 64)      0           add_9[0][0]                      \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 30, 64)       0           add_3[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_ff_norm (BatchNorma (None, 400, 64)      1600        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_ff_norm (BatchNorm (None, 30, 64)       120         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context_eeb_ff_ff (Dense)       (None, 400, 64)      4160        context_eeb_ff_norm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "question_eeb_ff_ff (Dense)      (None, 30, 64)       4160        question_eeb_ff_norm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 400, 64)      0           add_11[0][0]                     \n",
      "                                                                 context_eeb_ff_ff[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 30, 64)       0           add_5[0][0]                      \n",
      "                                                                 question_eeb_ff_ff[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 430, 64)      0           add_12[0][0]                     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 12000, 192)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (None, 12000, 1)     192         lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 400, 30)      0           time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 30, 400)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 400, 30)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, 30, 400)      0           lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 400, 400)     0           softmax_1[0][0]                  \n",
      "                                                                 softmax_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 400, 64)      0           softmax_1[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 400, 64)      0           dot_2[0][0]                      \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 400, 64)      0           add_12[0][0]                     \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 400, 64)      0           add_12[0][0]                     \n",
      "                                                                 dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 400, 256)     0           add_12[0][0]                     \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_resize (SeparableConv1D)   (None, 400, 64)      17728       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_conv_norm (None, 400, 64)      1600        conv_resize[0][0]                \n",
      "                                                                 add_24[0][0]                     \n",
      "                                                                 add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_conv_conv (None, 400, 64)      4480        model_encoder_block_0_conv_norm_0\n",
      "                                                                 model_encoder_block_0_conv_norm_0\n",
      "                                                                 model_encoder_block_0_conv_norm_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_conv_norm (None, 400, 64)      1600        model_encoder_block_0_conv_conv_0\n",
      "                                                                 model_encoder_block_0_conv_conv_0\n",
      "                                                                 model_encoder_block_0_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_conv_conv (None, 400, 64)      4480        model_encoder_block_0_conv_norm_1\n",
      "                                                                 model_encoder_block_0_conv_norm_1\n",
      "                                                                 model_encoder_block_0_conv_norm_1\n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 400, 64)      0           model_encoder_block_0_conv_conv_1\n",
      "                                                                 model_encoder_block_0_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_self_atte (None, 400, 64)      1600        add_13[0][0]                     \n",
      "                                                                 add_25[0][0]                     \n",
      "                                                                 add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 400, 400)     0           time_distributed_28[0][0]        \n",
      "                                                                 time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 400, 400)     0           time_distributed_31[0][0]        \n",
      "                                                                 time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 400, 400)     0           time_distributed_34[0][0]        \n",
      "                                                                 time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 400, 400)     0           time_distributed_37[0][0]        \n",
      "                                                                 time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 400, 400)     0           lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 400, 400)     0           lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 400, 400)     0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 400, 400)     0           lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 400, 400)     0           activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, 400, 64)      4096        model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 400, 64)      0           dropout_5[0][0]                  \n",
      "                                                                 time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 400, 64)      0           dropout_5[1][0]                  \n",
      "                                                                 time_distributed_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 400, 64)      0           dropout_5[2][0]                  \n",
      "                                                                 time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 400, 64)      0           dropout_5[3][0]                  \n",
      "                                                                 time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 400, 256)     0           lambda_24[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_40 (TimeDistri (None, 400, 64)      16448       concatenate_5[0][0]              \n",
      "                                                                 concatenate_8[0][0]              \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 400, 64)      0           time_distributed_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 400, 64)      0           dropout_8[0][0]                  \n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 400, 64)      0           add_13[0][0]                     \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_ff_norm ( (None, 400, 64)      1600        add_15[0][0]                     \n",
      "                                                                 add_27[0][0]                     \n",
      "                                                                 add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_0_ff_ff (De (None, 400, 64)      4160        model_encoder_block_0_ff_norm[0][\n",
      "                                                                 model_encoder_block_0_ff_norm[1][\n",
      "                                                                 model_encoder_block_0_ff_norm[2][\n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 400, 64)      0           add_15[0][0]                     \n",
      "                                                                 model_encoder_block_0_ff_ff[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_conv_norm (None, 400, 64)      1600        add_16[0][0]                     \n",
      "                                                                 add_28[0][0]                     \n",
      "                                                                 add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_conv_conv (None, 400, 64)      4480        model_encoder_block_1_conv_norm_0\n",
      "                                                                 model_encoder_block_1_conv_norm_0\n",
      "                                                                 model_encoder_block_1_conv_norm_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_conv_norm (None, 400, 64)      1600        model_encoder_block_1_conv_conv_0\n",
      "                                                                 model_encoder_block_1_conv_conv_0\n",
      "                                                                 model_encoder_block_1_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_conv_conv (None, 400, 64)      4480        model_encoder_block_1_conv_norm_1\n",
      "                                                                 model_encoder_block_1_conv_norm_1\n",
      "                                                                 model_encoder_block_1_conv_norm_1\n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 400, 64)      0           model_encoder_block_1_conv_conv_1\n",
      "                                                                 model_encoder_block_1_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_self_atte (None, 400, 64)      1600        add_17[0][0]                     \n",
      "                                                                 add_29[0][0]                     \n",
      "                                                                 add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_41 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_44 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_45 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_47 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_48 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_50 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_51 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 400, 400)     0           time_distributed_41[0][0]        \n",
      "                                                                 time_distributed_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 400, 400)     0           time_distributed_44[0][0]        \n",
      "                                                                 time_distributed_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 400, 400)     0           time_distributed_47[0][0]        \n",
      "                                                                 time_distributed_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 400, 400)     0           time_distributed_50[0][0]        \n",
      "                                                                 time_distributed_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 400, 400)     0           lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 400, 400)     0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 400, 400)     0           lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 400, 400)     0           lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 400, 400)     0           activation_13[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_43 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_46 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_49 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_52 (TimeDistri (None, 400, 64)      4096        model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 400, 64)      0           dropout_6[0][0]                  \n",
      "                                                                 time_distributed_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 400, 64)      0           dropout_6[1][0]                  \n",
      "                                                                 time_distributed_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 400, 64)      0           dropout_6[2][0]                  \n",
      "                                                                 time_distributed_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 400, 64)      0           dropout_6[3][0]                  \n",
      "                                                                 time_distributed_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 400, 256)     0           lambda_32[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_53 (TimeDistri (None, 400, 64)      16448       concatenate_6[0][0]              \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 400, 64)      0           time_distributed_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 400, 64)      0           dropout_9[0][0]                  \n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 400, 64)      0           add_17[0][0]                     \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_ff_norm ( (None, 400, 64)      1600        add_19[0][0]                     \n",
      "                                                                 add_31[0][0]                     \n",
      "                                                                 add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_1_ff_ff (De (None, 400, 64)      4160        model_encoder_block_1_ff_norm[0][\n",
      "                                                                 model_encoder_block_1_ff_norm[1][\n",
      "                                                                 model_encoder_block_1_ff_norm[2][\n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 400, 64)      0           add_19[0][0]                     \n",
      "                                                                 model_encoder_block_1_ff_ff[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_conv_norm (None, 400, 64)      1600        add_20[0][0]                     \n",
      "                                                                 add_32[0][0]                     \n",
      "                                                                 add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_conv_conv (None, 400, 64)      4480        model_encoder_block_2_conv_norm_0\n",
      "                                                                 model_encoder_block_2_conv_norm_0\n",
      "                                                                 model_encoder_block_2_conv_norm_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_conv_norm (None, 400, 64)      1600        model_encoder_block_2_conv_conv_0\n",
      "                                                                 model_encoder_block_2_conv_conv_0\n",
      "                                                                 model_encoder_block_2_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_conv_conv (None, 400, 64)      4480        model_encoder_block_2_conv_norm_1\n",
      "                                                                 model_encoder_block_2_conv_norm_1\n",
      "                                                                 model_encoder_block_2_conv_norm_1\n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 400, 64)      0           model_encoder_block_2_conv_conv_1\n",
      "                                                                 model_encoder_block_2_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_self_atte (None, 400, 64)      1600        add_21[0][0]                     \n",
      "                                                                 add_33[0][0]                     \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_54 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_55 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_57 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_58 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_60 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_61 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_63 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_64 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 400, 400)     0           time_distributed_54[0][0]        \n",
      "                                                                 time_distributed_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 400, 400)     0           time_distributed_57[0][0]        \n",
      "                                                                 time_distributed_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 400, 400)     0           time_distributed_60[0][0]        \n",
      "                                                                 time_distributed_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 400, 400)     0           time_distributed_63[0][0]        \n",
      "                                                                 time_distributed_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 400, 400)     0           lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 400, 400)     0           lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 400, 400)     0           lambda_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 400, 400)     0           lambda_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 400, 400)     0           activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_56 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_59 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_62 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_65 (TimeDistri (None, 400, 64)      4096        model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 400, 64)      0           dropout_7[0][0]                  \n",
      "                                                                 time_distributed_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 400, 64)      0           dropout_7[1][0]                  \n",
      "                                                                 time_distributed_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 400, 64)      0           dropout_7[2][0]                  \n",
      "                                                                 time_distributed_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 400, 64)      0           dropout_7[3][0]                  \n",
      "                                                                 time_distributed_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 400, 256)     0           lambda_40[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_44[0][0]                  \n",
      "                                                                 lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_66 (TimeDistri (None, 400, 64)      16448       concatenate_7[0][0]              \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 400, 64)      0           time_distributed_66[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 400, 64)      0           dropout_10[0][0]                 \n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 400, 64)      0           add_21[0][0]                     \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_ff_norm ( (None, 400, 64)      1600        add_23[0][0]                     \n",
      "                                                                 add_35[0][0]                     \n",
      "                                                                 add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_encoder_block_2_ff_ff (De (None, 400, 64)      4160        model_encoder_block_2_ff_norm[0][\n",
      "                                                                 model_encoder_block_2_ff_norm[1][\n",
      "                                                                 model_encoder_block_2_ff_norm[2][\n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 400, 64)      0           add_23[0][0]                     \n",
      "                                                                 model_encoder_block_2_ff_ff[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 400, 64)      0           model_encoder_block_0_conv_conv_1\n",
      "                                                                 model_encoder_block_0_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 400, 400)     0           time_distributed_28[1][0]        \n",
      "                                                                 time_distributed_29[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 400, 400)     0           time_distributed_31[1][0]        \n",
      "                                                                 time_distributed_32[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 400, 400)     0           time_distributed_34[1][0]        \n",
      "                                                                 time_distributed_35[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 400, 400)     0           time_distributed_37[1][0]        \n",
      "                                                                 time_distributed_38[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 400, 400)     0           lambda_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 400, 400)     0           lambda_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 400, 400)     0           lambda_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 400, 400)     0           lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 400, 64)      0           dropout_5[4][0]                  \n",
      "                                                                 time_distributed_30[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 400, 64)      0           dropout_5[5][0]                  \n",
      "                                                                 time_distributed_33[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 400, 64)      0           dropout_5[6][0]                  \n",
      "                                                                 time_distributed_36[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 400, 64)      0           dropout_5[7][0]                  \n",
      "                                                                 time_distributed_39[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 400, 256)     0           lambda_48[0][0]                  \n",
      "                                                                 lambda_50[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 400, 64)      0           time_distributed_40[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 400, 64)      0           dropout_11[0][0]                 \n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 400, 64)      0           add_25[0][0]                     \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 400, 64)      0           add_27[0][0]                     \n",
      "                                                                 model_encoder_block_0_ff_ff[1][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 400, 64)      0           model_encoder_block_1_conv_conv_1\n",
      "                                                                 model_encoder_block_1_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 400, 400)     0           time_distributed_41[1][0]        \n",
      "                                                                 time_distributed_42[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 400, 400)     0           time_distributed_44[1][0]        \n",
      "                                                                 time_distributed_45[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 400, 400)     0           time_distributed_47[1][0]        \n",
      "                                                                 time_distributed_48[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 400, 400)     0           time_distributed_50[1][0]        \n",
      "                                                                 time_distributed_51[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 400, 400)     0           lambda_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 400, 400)     0           lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 400, 400)     0           lambda_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 400, 400)     0           lambda_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 400, 64)      0           dropout_6[4][0]                  \n",
      "                                                                 time_distributed_43[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 400, 64)      0           dropout_6[5][0]                  \n",
      "                                                                 time_distributed_46[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 400, 64)      0           dropout_6[6][0]                  \n",
      "                                                                 time_distributed_49[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 400, 64)      0           dropout_6[7][0]                  \n",
      "                                                                 time_distributed_52[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 400, 256)     0           lambda_56[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 400, 64)      0           time_distributed_53[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 400, 64)      0           dropout_12[0][0]                 \n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 400, 64)      0           add_29[0][0]                     \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 400, 64)      0           add_31[0][0]                     \n",
      "                                                                 model_encoder_block_1_ff_ff[1][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 400, 64)      0           model_encoder_block_2_conv_conv_1\n",
      "                                                                 model_encoder_block_2_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 400, 400)     0           time_distributed_54[1][0]        \n",
      "                                                                 time_distributed_55[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 400, 400)     0           time_distributed_57[1][0]        \n",
      "                                                                 time_distributed_58[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 400, 400)     0           time_distributed_60[1][0]        \n",
      "                                                                 time_distributed_61[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 400, 400)     0           time_distributed_63[1][0]        \n",
      "                                                                 time_distributed_64[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 400, 400)     0           lambda_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 400, 400)     0           lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 400, 400)     0           lambda_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 400, 400)     0           lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 400, 64)      0           dropout_7[4][0]                  \n",
      "                                                                 time_distributed_56[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 400, 64)      0           dropout_7[5][0]                  \n",
      "                                                                 time_distributed_59[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 400, 64)      0           dropout_7[6][0]                  \n",
      "                                                                 time_distributed_62[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 400, 64)      0           dropout_7[7][0]                  \n",
      "                                                                 time_distributed_65[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 400, 256)     0           lambda_64[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_68[0][0]                  \n",
      "                                                                 lambda_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 400, 64)      0           time_distributed_66[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 400, 64)      0           dropout_13[0][0]                 \n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 400, 64)      0           add_33[0][0]                     \n",
      "                                                                 add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 400, 64)      0           add_35[0][0]                     \n",
      "                                                                 model_encoder_block_2_ff_ff[1][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 400, 64)      0           model_encoder_block_0_conv_conv_1\n",
      "                                                                 model_encoder_block_0_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 400, 400)     0           time_distributed_28[2][0]        \n",
      "                                                                 time_distributed_29[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 400, 400)     0           time_distributed_31[2][0]        \n",
      "                                                                 time_distributed_32[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 400, 400)     0           time_distributed_34[2][0]        \n",
      "                                                                 time_distributed_35[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 400, 400)     0           time_distributed_37[2][0]        \n",
      "                                                                 time_distributed_38[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 400, 400)     0           lambda_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 400, 400)     0           lambda_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 400, 400)     0           lambda_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 400, 400)     0           lambda_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 400, 64)      0           dropout_5[8][0]                  \n",
      "                                                                 time_distributed_30[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 400, 64)      0           dropout_5[9][0]                  \n",
      "                                                                 time_distributed_33[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 400, 64)      0           dropout_5[10][0]                 \n",
      "                                                                 time_distributed_36[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 400, 64)      0           dropout_5[11][0]                 \n",
      "                                                                 time_distributed_39[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 400, 256)     0           lambda_72[0][0]                  \n",
      "                                                                 lambda_74[0][0]                  \n",
      "                                                                 lambda_76[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 400, 64)      0           time_distributed_40[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 400, 64)      0           dropout_14[0][0]                 \n",
      "                                                                 model_encoder_block_0_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 400, 64)      0           add_37[0][0]                     \n",
      "                                                                 add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 400, 64)      0           add_39[0][0]                     \n",
      "                                                                 model_encoder_block_0_ff_ff[2][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 400, 64)      0           model_encoder_block_1_conv_conv_1\n",
      "                                                                 model_encoder_block_1_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 400, 400)     0           time_distributed_41[2][0]        \n",
      "                                                                 time_distributed_42[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 400, 400)     0           time_distributed_44[2][0]        \n",
      "                                                                 time_distributed_45[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 400, 400)     0           time_distributed_47[2][0]        \n",
      "                                                                 time_distributed_48[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 400, 400)     0           time_distributed_50[2][0]        \n",
      "                                                                 time_distributed_51[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 400, 400)     0           lambda_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 400, 400)     0           lambda_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 400, 400)     0           lambda_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 400, 400)     0           lambda_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 400, 64)      0           dropout_6[8][0]                  \n",
      "                                                                 time_distributed_43[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 400, 64)      0           dropout_6[9][0]                  \n",
      "                                                                 time_distributed_46[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 400, 64)      0           dropout_6[10][0]                 \n",
      "                                                                 time_distributed_49[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 400, 64)      0           dropout_6[11][0]                 \n",
      "                                                                 time_distributed_52[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 400, 256)     0           lambda_80[0][0]                  \n",
      "                                                                 lambda_82[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 400, 64)      0           time_distributed_53[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 400, 64)      0           dropout_15[0][0]                 \n",
      "                                                                 model_encoder_block_1_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 400, 64)      0           add_41[0][0]                     \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 400, 64)      0           add_43[0][0]                     \n",
      "                                                                 model_encoder_block_1_ff_ff[2][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 400, 64)      0           model_encoder_block_2_conv_conv_1\n",
      "                                                                 model_encoder_block_2_conv_conv_0\n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 400, 400)     0           time_distributed_54[2][0]        \n",
      "                                                                 time_distributed_55[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 400, 400)     0           time_distributed_57[2][0]        \n",
      "                                                                 time_distributed_58[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 400, 400)     0           time_distributed_60[2][0]        \n",
      "                                                                 time_distributed_61[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 400, 400)     0           time_distributed_63[2][0]        \n",
      "                                                                 time_distributed_64[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 400, 400)     0           lambda_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 400, 400)     0           lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 400, 400)     0           lambda_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 400, 400)     0           lambda_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 400, 64)      0           dropout_7[8][0]                  \n",
      "                                                                 time_distributed_56[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 400, 64)      0           dropout_7[9][0]                  \n",
      "                                                                 time_distributed_59[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 400, 64)      0           dropout_7[10][0]                 \n",
      "                                                                 time_distributed_62[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, 400, 64)      0           dropout_7[11][0]                 \n",
      "                                                                 time_distributed_65[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 400, 256)     0           lambda_88[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "                                                                 lambda_92[0][0]                  \n",
      "                                                                 lambda_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 400, 64)      0           time_distributed_66[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 400, 64)      0           dropout_16[0][0]                 \n",
      "                                                                 model_encoder_block_2_self_attent\n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 400, 64)      0           add_45[0][0]                     \n",
      "                                                                 add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 400, 64)      0           add_47[0][0]                     \n",
      "                                                                 model_encoder_block_2_ff_ff[2][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 400, 128)     0           add_24[0][0]                     \n",
      "                                                                 add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 400, 128)     0           add_24[0][0]                     \n",
      "                                                                 add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_67 (TimeDistri (None, 400, 1)       128         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_68 (TimeDistri (None, 400, 1)       128         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           time_distributed_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 400)          0           time_distributed_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 400)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_4 (Softmax)             (None, 400)          0           flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 854,952\n",
      "Trainable params: 840,192\n",
      "Non-trainable params: 14,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[context_input,question_input] ,outputs =[start_output,end_output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e6pXkV9H01lW",
    "outputId": "77a84765-233f-433a-bace-ad3a512d40bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence\n"
     ]
    }
   ],
   "source": [
    "# Parametros\n",
    "BATCH_SIZE=8\n",
    "EPOCHS=50\n",
    "OPTIMIZER='adam'\n",
    "LOSS= 'categorical_crossentropy'\n",
    "generator= TensorSequence(train,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='weights.hdf5',monitor=\"loss\", verbose=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETURNING a batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 400, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2yvpN5dn01lY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=OPTIMIZER,loss=LOSS, metrics=['accuracy'])\n",
    "model.fit_generator(generator, steps_per_epoch = TRAIN_COUNT//BATCH_SIZE, max_queue_size=5, epochs = EPOCHS, verbose=2, callbacks=callbacks_list, use_multiprocessing=True, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PM54IPNo01lZ"
   },
   "outputs": [],
   "source": [
    "#entrenamos desde archivo guardado\n",
    "\n",
    "model = load_model('weights.hdf5')\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=50\n",
    "OPTIMIZER= Adam()\n",
    "LOSS= 'categorical_crossentropy'\n",
    "generator= TensorSequence(train,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='weights.hdf5',monitor=\"loss\", verbose=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "L7ZcGcSS01lc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=OPTIMIZER,loss=LOSS, metrics=['accuracy'])\n",
    "model.fit_generator(generator, steps_per_epoch = TRAIN_COUNT//BATCH_SIZE, max_queue_size=5, epochs = EPOCHS, verbose=1, callbacks=callbacks_list, use_multiprocessing=True, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnLqO4zk01ld"
   },
   "source": [
    " ## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-HPN85o01le"
   },
   "source": [
    "Si bien no se alcanzó a hacer un entrenamiento apropiado de todas formas se intentará evaluar el modelo.\n",
    "\n",
    "Por tiempo generaremos arreglo con indices originales y arreglo on indices predecidos y usaremos el metodo de sklearn para calcular el fscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-HEVBmDE01le",
    "outputId": "ce723e1d-4458-4a89-8451-6c35fddb4567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence\n"
     ]
    }
   ],
   "source": [
    "# generador de test data.\n",
    "test_generator = TensorSequence(test,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "d8hUMrql01lg",
    "outputId": "0d9bb135-4bbe-4897-b2a1-d634bad3ef4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 677)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input,output=test_generator[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zBn9zQ9I01li"
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "j = 0\n",
    "\n",
    "# Predecimos por batch\n",
    "for k in range(len(test_generator)):\n",
    "    print(k,len(test_generator))\n",
    "    batch = test_generator[k]\n",
    "    for i in range(len(batch[1][0])):\n",
    "        # agregamos la tupla real de inicio y fin.\n",
    "        y_true.append((np.argmax(batch[1][0][i]),np.argmax(batch[1][1][i])))\n",
    "    predict = model.predict_on_batch(batch[0])\n",
    "    for i in range(len(predict[1])):\n",
    "        #agregamos la tupla predecida.\n",
    "        y_pred.append((np.argmax(predict[0][i]),np.argmax(predict[1][i])))\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "s_OZnJoM01lk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Mapeamos a arreglos 1 dimensionales para insertar en la función de sklearn.\n",
    "\n",
    "y_true_start = list(map(lambda x: x[0], y_true))\n",
    "y_pred_start = list(map(lambda x: x[0], y_pred))\n",
    "\n",
    "y_true_end = list(map(lambda x: x[1], y_true))\n",
    "y_pred_end = list(map(lambda x: x[1], y_pred))\n",
    "\n",
    "y_true_1d = list(map(lambda x: \"{0} {1}\".format(*x), y_true))\n",
    "y_pred_1d = list(map(lambda x: \"{0} {1}\".format(*x), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B5dO7SW301ll"
   },
   "outputs": [],
   "source": [
    "print(\"f1 score inicio respuesta: {}\".format(f1_score(y_true_start, y_pred_start, average='micro')*100))\n",
    "print(\"f1 score fin respuesta: {}\".format(f1_score(y_true_end, y_pred_end, average='micro')*100))\n",
    "print(\"f1 score concatenacion: {}\".format(f1_score(y_true_1d, y_pred_1d, average='micro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GRPuBXyD01lo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1530685981785,
     "user": {
      "displayName": "Freddie VENEGAS APABLAZA",
      "photoUrl": "//lh5.googleusercontent.com/-AV4JSEifrxo/AAAAAAAAAAI/AAAAAAAAAHg/mfr-4hyuy7Y/s50-c-k-no/photo.jpg",
      "userId": "109502576005160238836"
     },
     "user_tz": 240
    },
    "id": "JvPPJFaB_WyL",
    "outputId": "6511ff0e-bd63-4d9c-cfbc-78f253d0364d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1530699429972,
     "user": {
      "displayName": "Freddie VENEGAS APABLAZA",
      "photoUrl": "//lh5.googleusercontent.com/-AV4JSEifrxo/AAAAAAAAAAI/AAAAAAAAAHg/mfr-4hyuy7Y/s50-c-k-no/photo.jpg",
      "userId": "109502576005160238836"
     },
     "user_tz": 240
    },
    "id": "FqmfIDqVC0ZM",
    "outputId": "05a0dfc6-568f-4499-ac60-4287149afe30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21194157, 0.5761169 , 0.21194157],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.activations import softmax\n",
    "import keras.backend as K\n",
    "\n",
    "a = np.array([[1,2,1], [2,3,100], [0, 100, 1]])\n",
    "K.eval(softmax(K.variable(a),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 725,
     "status": "error",
     "timestamp": 1530695844697,
     "user": {
      "displayName": "Freddie VENEGAS APABLAZA",
      "photoUrl": "//lh5.googleusercontent.com/-AV4JSEifrxo/AAAAAAAAAAI/AAAAAAAAAHg/mfr-4hyuy7Y/s50-c-k-no/photo.jpg",
      "userId": "109502576005160238836"
     },
     "user_tz": 240
    },
    "id": "OtzTZAGIlKsq",
    "outputId": "b75aa594-5c7d-4db1-9c1d-15ab52380e98"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8ce765dcfa30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (3,2) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "24aHKZm9iMIJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "main.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
