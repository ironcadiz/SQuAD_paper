{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Preguntas y Respuestas en Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para el preprocesamiento haremos las siguientes cosas:\n",
    "\n",
    "1. Pasar preguntas y contextos a arreglos de palabras.\n",
    "2. A partir campo `answer_start` crear un indice de palabra y no de caracter `answer_word_start`.\n",
    "3. Agregar campo `answer_word_end`\n",
    "\n",
    "Guardaremos el archivo en un JSON nuevo para no tener que repetir el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar dependencias para el preprocesamiento.\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train-v1.1.json\", \"r\") as data:\n",
    "    train = json.load(data)['data']\n",
    "with open(\"dev-v1.1.json\", \"r\") as data:\n",
    "    test = json.load(data)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    for document in data:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            preprocess_paragraph(paragraph)\n",
    "\n",
    "def preprocess_paragraph(paragraph):\n",
    "    # preprocesamos contexto\n",
    "    preprocess_context(paragraph)\n",
    "    for question in paragraph['qas']:\n",
    "        # preprocesamos preguntas.\n",
    "        preprocess_question(paragraph['context'],question)\n",
    "\n",
    "def preprocess_context(paragraph):\n",
    "\n",
    "    # Guardamos contexto como arreglo preprocesado\n",
    "    paragraph['context_tokenized'] =  preprocess_text(paragraph['context'])\n",
    "\n",
    "def preprocess_question(context,question):\n",
    "    \n",
    "    # guardamos pregunta como arreglo\n",
    "    question['question_tokenized'] = preprocess_text(question['question'])\n",
    "    for answer in question['answers']:\n",
    "        # preprocesamos respuestas\n",
    "        preprocess_answer(context, answer)\n",
    "    \n",
    "def preprocess_answer(context,answer):\n",
    "    \n",
    "    # Pasamos respuesta a arreglo\n",
    "    answer['text_tokenized'] = preprocess_text(answer['text'])\n",
    "    \n",
    "\n",
    "    # Contamos cantidad de palabras hasta la respuesta\n",
    "    answer_word = len(preprocess_text(context[:answer['answer_start']]))\n",
    "    \n",
    "    # Guardamos en el hash\n",
    "    answer['answer_word_start'] = answer_word\n",
    "    \n",
    "    # Guardamos la palabra final de la respusta en el hash.\n",
    "    answer['answer_word_end'] = answer_word + len(answer['text_tokenized']) - 1\n",
    "    \n",
    "\"\"\"\n",
    "retorna string tokenizado y limpio de simbolos y mayusculas. Esto ultimo es necesario para disminuir\n",
    "las veces en que GLove no tiene la palabra.\n",
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    result = text\n",
    "    \n",
    "    # minusculas\n",
    "    result = result.lower()\n",
    "    \n",
    "    # simbolos para eliminar\n",
    "    symbols = re.sub(\"[{}]\".format(string.ascii_letters + \"'1234567890\" ),\"\",string.printable)\n",
    "    \n",
    "    # eliminamos simbolos mediante regexp.\n",
    "    result = re.sub(\"[{}–]\".format(symbols),\" \", result)\n",
    "    \n",
    "    return word_tokenize(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamos y guardamos el nuevo dataset\n",
    "with open(\"train-v1.1-pr.json\",\"w\") as out:\n",
    "    print(\"preprocessing training set\")\n",
    "    preprocess_dataset(train)\n",
    "    json.dump(train,out)\n",
    "    \n",
    "with open(\"dev-v1.1-pr.json\",\"w\") as out:\n",
    "    print(\"preprocessing test set\")\n",
    "    preprocess_dataset(test)\n",
    "    json.dump(test,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Para usar los embeddings e inyectarlos en el modelo de Keras primero se intento lo siguiente:\n",
    "\n",
    "1. Construiremos un index de palabras a partir del vocabulario del dataset.\n",
    "2. Transformamos las secuencias de palabras en secuencias de enteros mediante el index.\n",
    "3. Estandarizamos el Tamaño de la secuencia usando `pad_sequences` de Keras.\n",
    "4. Luego  construimos matriz de pesos a partir de Glove para inyectar a una capa `Embedding` de Keras.\n",
    "\n",
    "Este proceso no funcionó porque los vectores de glove ocupaban demasiada memoria de la gpu (eran 50M de parametros), a cambio se optó por precomputar los vectores de cada palabra y pasar los tensores de una sequencia directamente a la red. Como el dataset crece demasiado al pasar cada palabra a un vector de 300D, esta transformación se genera de a poco mediante un objeto Keras Sequence ( se probó primero un generador pero no era compatible con el paralelismo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos dependencias para generar los embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import Sequence\n",
    "import threading\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running script\n",
      "Loading Glove Vectors\n"
     ]
    }
   ],
   "source": [
    "# Usamos los glove vectors 300D de wikipedia 2014. Por limitación de memoria no podemos usar un corpus más grande.\n",
    "glove_file = datapath(os.getcwd() + '/glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(os.getcwd() + \"/test_word2vec.txt\")\n",
    "print(\"Running script\")\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "print(\"Loading Glove Vectors\")\n",
    "embedder = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para metodo Embedding layer (se desecho)\n",
    "# retorna secuencia de enteros a partir de secuencia de palabras.\n",
    "def text_to_sequence(text_seq, word_index):\n",
    "    result = []\n",
    "    for word in text_seq:\n",
    "        if word in word_index:\n",
    "            result.append(word_index[word])\n",
    "        else:\n",
    "            word_index[word] = len(word_index)\n",
    "            result.append(word_index[word])\n",
    "    return np.array(result)\n",
    "\n",
    "# Construimos index de palabras a medida que aparecen en el dataset. \n",
    "# Construimos matrices de enteros a partir del dataset.\n",
    "def gen_data(dataset,word_index):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    output = []\n",
    "    for document in dataset:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                # Tomamos la primera respuesta para generar el dataset final\n",
    "                answer = question['answers'][0]\n",
    "                # Pasamos secuencias a secuencias de enteros.\n",
    "                contexts.append(text_to_sequence(paragraph['context_tokenized'],word_index))\n",
    "                questions.append(text_to_sequence(question['question_tokenized'],word_index))\n",
    "                #guardamos tupla de inicio y fin para el output.\n",
    "                output.append((answer['answer_word_start'],answer['answer_word_end']))\n",
    "    return contexts,questions,output,word_index\n",
    "\n",
    "\n",
    "\n",
    "# Funciones para la keras sequence.\n",
    "\n",
    "# Pasamos una secuencia de texto a un tensor de tamaño fijo.\n",
    "# Quedan en 0 el padding y las palabras que no estan en GLove\n",
    "def text_to_tensor(text_seq, word_vectors,sequence_length):\n",
    "    result = np.zeros((sequence_length,300))\n",
    "    for i,t in enumerate(text_seq):\n",
    "        if t in word_vectors:\n",
    "            result[i]  = word_vectors[t]\n",
    "    return result\n",
    "\n",
    "def data_counter(dataset):\n",
    "    count = 0\n",
    "    for document in dataset:\n",
    "        for paragraph in document['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                count +=1\n",
    "    return count\n",
    "\n",
    "# Secuencia para hacer multijob data generation\n",
    "class TensorSequence(Sequence):\n",
    "\n",
    "    def __init__(self, dataset,batch_size,word_vectors,context_length,question_length):\n",
    "        self.dataset = []\n",
    "        self.batch_size = batch_size\n",
    "        self.data_count = data_counter(dataset)\n",
    "        self.word_vectors = word_vectors\n",
    "        self.context_length = context_length\n",
    "        self.question_length = question_length\n",
    "        print(\"Loading sequence\")\n",
    "        # Guardamos el dataset en formato tabla para poder indexar por batch size.\n",
    "        for document in dataset:\n",
    "            for paragraph in document['paragraphs']:\n",
    "                for question in paragraph['qas']:\n",
    "                    # Tomamos la primera respuesta para generar el dataset final\n",
    "                    answer = question['answers'][0]\n",
    "                    self.dataset.append([paragraph['context_tokenized'],question['question_tokenized'],answer['answer_word_start'],answer['answer_word_end']])\n",
    "\n",
    "    # steps per batch\n",
    "    def __len__(self):\n",
    "        return self.data_count//self.batch_size\n",
    "\n",
    "    \n",
    "    \n",
    "    # retorna un batch de tensores.\n",
    "    def __getitem__(self, idx):\n",
    "        contexts = None\n",
    "        questions = None\n",
    "        output_start = []\n",
    "        output_end = []\n",
    "        #iteramos sobre el batch pedido.\n",
    "        for row in self.dataset[idx * self.batch_size:(idx + 1) * self.batch_size]:\n",
    "            \n",
    "            # pasamos sequencias de palabras a tensores\n",
    "            if contexts is None:\n",
    "                contexts = text_to_tensor(row[0],self.word_vectors,self.context_length)\n",
    "            else:\n",
    "                contexts = np.dstack((contexts,text_to_tensor(row[0],self.word_vectors,self.context_length)))\n",
    "            if  questions is None:\n",
    "                questions = text_to_tensor(row[1],self.word_vectors,self.question_length)\n",
    "            else:\n",
    "                questions= np.dstack((questions,text_to_tensor(row[1],self.word_vectors,self.question_length)))\n",
    "\n",
    "            #guardamos tupla de inicio y fin para el output.\n",
    "            output_start.append(row[2])\n",
    "            output_end.append(row[3])\n",
    "        return [np.moveaxis(contexts,2,0),np.moveaxis(questions,2,0)],[to_categorical(np.array(output_start),num_classes=self.context_length),to_categorical(np.array(output_end),num_classes=self.context_length)]\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generamos las secuencias de enteros para inyectar en el modelo de Keras.\n",
    "with open(\"train-v1.1-pr.json\", \"r\") as data:\n",
    "    train = json.load(data)\n",
    "with open(\"dev-v1.1-pr.json\", \"r\") as data:\n",
    "    test = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el tamaño máximo\n",
    "def max_context(document):\n",
    "    return max(document['paragraphs'], key= lambda x: len(x['context_tokenized']))\n",
    "\n",
    "def max_question_par(paragraph):\n",
    "    return max(paragraph['qas'], key= lambda x: len(x['question_tokenized']))\n",
    "\n",
    "def max_paragraph(document):\n",
    "    return max(document['paragraphs'], key=lambda x: len(max_question_par(x)['question_tokenized']))\n",
    "\n",
    "MAX_CONTEXT = len(max(map(lambda x: max_context(x), train), key=lambda x:len(x['context_tokenized']))['context_tokenized'])\n",
    "MAX_QUESTIONS = len(max_question_par(max_paragraph(max(train, key= lambda x:  len(max_question_par(max_paragraph(x))['question_tokenized']))))['question_tokenized'])\n",
    "TRAIN_COUNT = data_counter(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagrama del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo ocupado está inspirado en el trabajo de Raiman y Miller, \"Globally Normalized Reader\" (2017). Los pesos del modelo fueron omitidos en el diagrama por temas de visualización. Este modelo cuenta con dos etapas iniciales que se procesan por separado:\n",
    "\n",
    "*Nota*: el subíndice $T$ podría ser diferente entre la codificación de la pregunta, el contexto o el input de la mezcla de ambos. \n",
    "\n",
    "- **Question Input**: la pregunta, luego de ser codificada usando GloVe, pasa por tres capas LSTM Bidireccionales. En el diagrama, el rectángulo gris que rodea cada unidad (representadas con la letra $h$, en los círculos de color azul) de las capas bidireccionales, representa una concatenación (tal como lo hace el modelo clásico de RNN bidireccional). El output de la capa Bidireccional final pasa por una capa densa con activación ReLu. La separación de la capa densa por unidades relacionadas con cada unidad de la última LSTM Bidireccional y su posterior concatenación, es para hacer explícito el procedimiento que hace keras por detrás. En el código esto no se ve reflejado explícitamente.\n",
    "\n",
    "- **Context Input**: el contexto, luego de ser debidamente codificado con GloVe, pasa por solo una capa LSTM bidireccional para no aumentar demasiado el tamaño del modelo. El output de esta capa es concatenado con el output de la pregunta.\n",
    "\n",
    "La siguiente etapa incluye la información tanto de la pregunta como del contexto. Esta pasa por dos capas LSTM Bidireccionales.\n",
    "\n",
    "\n",
    "\n",
    "En el siguiente paso se generan dos outputs:\n",
    "\n",
    "1) Output de la última unidad de la última capa bidireccional, el cual va hacia una capa densa con activación softmax.\n",
    "\n",
    "2) Outputs de todas las unidades de la última capa bidireccional, los cuales son concatenados.\n",
    "\n",
    "A partir de la primera softmax se obtiene la distribución de probabilidad el índice de la primera palabra de la respuesta, que esta codificado mediante un vector `one-hot`.\n",
    "\n",
    "Luego, se hace una multiplicación _element-wise_ entre los outputs descritos (el primer output es concatenado con si mismo para que calcen las dimensiones en la multiplicación), de esta manera se condiciona lo que viene de aquí en adelante, por la probabilidad de la palabra inicial de la respuesta.\n",
    "\n",
    "Finalmente, este resultado pasa por una última capa densa con activación softmax para realizar la predicción del índice final de la respuesta que esta codificado de forma análoga.\n",
    "\n",
    "La arquitectura posee como hiperparametros la dimensión de la capa oculta de LSTMs para las preguntas y para el contexto.\n",
    "\n",
    "La red posee 7M de parametros y por lo tanto una capacidad considerable. A modo de regularización las LSTMs tienen dropout con probabilidad 0.3\n",
    "\n",
    "No hubo tiempo para experimentar mucho puesto que el modelo se demora bastante en entrenar. Se probaron 2 arquitecturas similares pero con más capas, que resultaban en modelos demasiado grandes. También se intento usar una capa de `Masking` para los 0 (palabras no encontradas o padding), pero al menos en los cortos experimentos que hicimos parecía ralentizar el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaración del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos dependencias\n",
    "from keras.layers import Embedding, Input, Concatenate,Bidirectional,LSTM, Dense, Reshape, TimeDistributed, Activation, Flatten,Multiply,Conv1D, Masking\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Para elegir GPU o multicore\n",
    "num_cores = 4\n",
    "CPU= False\n",
    "GPU= not CPU\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "if CPU:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PARAMS\n",
    "\n",
    "CONTEXT_RECURRENT_DIM=100\n",
    "QUESTION_RECURRENT_DIM=40\n",
    "\n",
    "# Input de contexto \n",
    "context_input = Input(shape=(MAX_CONTEXT,300))\n",
    "\n",
    "#Input de preguntas\n",
    "question_input = Input(shape=(MAX_QUESTIONS,300))\n",
    "\n",
    "\n",
    "# 3 capas de BiLSTM para las preguntas\n",
    "question_nw = Bidirectional(LSTM(units=QUESTION_RECURRENT_DIM,dropout=0.3,return_sequences=True))(question_input)\n",
    "question_nw = Bidirectional(LSTM(units=QUESTION_RECURRENT_DIM,dropout=0.3,return_sequences=True))(question_nw)\n",
    "question_nw = Bidirectional(LSTM(units=QUESTION_RECURRENT_DIM,dropout=0.3,return_sequences=True))(question_nw)\n",
    "\n",
    "# generamos vectores concatenables a los del contexto mediante capa densa.\n",
    "question_nw = Dense(units=2*CONTEXT_RECURRENT_DIM, activation='relu')(question_nw)\n",
    "\n",
    "# Codificacion inicial del contexto\n",
    "context_nw = Bidirectional(LSTM(units=CONTEXT_RECURRENT_DIM,dropout=0.3,return_sequences=True,))(context_input)\n",
    "\n",
    "#Concatenamos vectores de las preguntas a los vectores de las respuestas\n",
    "result = Concatenate(axis=1)([context_nw,question_nw])\n",
    "\n",
    "# Agregamos dos capas de BiLSTM\n",
    "doc_encoding = Bidirectional(LSTM(units=CONTEXT_RECURRENT_DIM,dropout=0.3,return_sequences=True))(result)\n",
    "doc_encoding = Bidirectional(LSTM(units=MAX_CONTEXT,dropout=0.3))(doc_encoding)\n",
    "\n",
    "doc_dense = Dense(MAX_CONTEXT)(doc_encoding)\n",
    "result_start = Activation(activation='softmax')(doc_dense)\n",
    "\n",
    "# Ahora hacemos un producto elementwise entre la softmax y los vectores de obtenidos de cada palabra\n",
    "# Asi condicionamos la palabra final por la distribución de la palabra inicial.\n",
    "\n",
    "result_prod = Concatenate(axis=1)([result_start,result_start])\n",
    "result_end = Multiply()([result_prod,doc_encoding])\n",
    "\n",
    "# Capa Densa y activacion final\n",
    "result_end = Dense(MAX_CONTEXT)(result_end)\n",
    "result_end = Activation(activation='softmax')(result_end)\n",
    "\n",
    "model = Model(inputs=[context_input,question_input],outputs=[result_start,result_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Se entrenó en una máquina con 8Gb de RAM, 4 Cores de CPU y una GTX1050M de 4Gb.\n",
    "\n",
    "Se utilizo `Adam` con `batch_size` de `32`, `learning_rate` `0.001` y categorical cross entropy como función de pérdida, sumando las pérdidas de la palabra inicial y la palabra final de la respuesta.\n",
    "\n",
    "Al entrenar no se paso un set de validación puesto que la memoria era un limitante fuerte y simplemente no había espacio. Es más se redujo el tamaño de la cola de batches a 5.\n",
    "\n",
    "Finalmente aprovechando que la generación de batches se puede paralelizar mediante una `keras.utils.Sequence`, se usan 3 workers para alimentar el entrenamiento.\n",
    "\n",
    "Luego de mucho optimizar e iterar se logró bajar el tiempo de entrenamiento a 3.5 horas por época,  lo que se mantuvo por alrededor de 16 horas llegando a 5 épocas, con un training accuracy de 4% y 2% para las palabras inicial y final.\n",
    "\n",
    "Luego se paro el entrenamiento y se volvió a iniciar esta véz con un learning rate mas alto (`0.003`), lo cual funcionó bien inicialmente hasta que en algun momento el gradiente explotó y el accuracy bajo estrepitosamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=50\n",
    "OPTIMIZER='adam'\n",
    "LOSS= 'categorical_crossentropy'\n",
    "generator= TensorSequence(train,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='weights.hdf5',monitor=\"loss\", verbose=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=OPTIMIZER,loss=LOSS, metrics=['accuracy'])\n",
    "model.fit_generator(generator, steps_per_epoch = TRAIN_COUNT//BATCH_SIZE, max_queue_size=5, epochs = EPOCHS, verbose=1, callbacks=callbacks_list, use_multiprocessing=True, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 80)       109120      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 40, 80)       38720       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 677, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 40, 80)       38720       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 677, 200)     320800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 40, 200)      16200       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 717, 200)     0           bidirectional_4[0][0]            \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 717, 200)     240800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1354)         4755248     bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 677)          917335      bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 677)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1354)         0           activation_1[0][0]               \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1354)         0           concatenate_2[0][0]              \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 677)          917335      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 677)          0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,354,278\n",
      "Trainable params: 7,354,278\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#entrenamos desde archivo guardado\n",
    "\n",
    "model = load_model('weights.hdf5')\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=50\n",
    "OPTIMIZER= Adam(lr=0.003) #nuevo learning rate\n",
    "LOSS= 'categorical_crossentropy'\n",
    "generator= TensorSequence(train,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='weights.hdf5',monitor=\"loss\", verbose=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2737/2737 [==============================] - 13657s 5s/step - loss: 9.9162 - activation_1_loss: 4.9658 - activation_2_loss: 4.9504 - activation_1_acc: 0.0341 - activation_2_acc: 0.0154\n",
      "\n",
      "Epoch 00001: saving model to weights.hdf5\n",
      "Epoch 2/50\n",
      "2737/2737 [==============================] - 13703s 5s/step - loss: 15.9773 - activation_1_loss: 11.0190 - activation_2_loss: 4.9583 - activation_1_acc: 0.0171 - activation_2_acc: 0.0150\n",
      "\n",
      "Epoch 00002: saving model to weights.hdf5\n",
      "Epoch 3/50\n",
      "2522/2737 [==========================>...] - ETA: 17:54 - loss: 21.0849 - activation_1_loss: 16.1161 - activation_2_loss: 4.9689 - activation_1_acc: 9.9128e-05 - activation_2_acc: 0.0149"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=OPTIMIZER,loss=LOSS, metrics=['accuracy'])\n",
    "model.fit_generator(generator, steps_per_epoch = TRAIN_COUNT//BATCH_SIZE, max_queue_size=5, epochs = EPOCHS, verbose=1, callbacks=callbacks_list, use_multiprocessing=True, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien no se alcanzó a hacer un entrenamiento apropiado de todas formas se intentará evaluar el modelo.\n",
    "\n",
    "Por tiempo generaremos arreglo con indices originales y arreglo on indices predecidos y usaremos el metodo de sklearn para calcular el fscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence\n"
     ]
    }
   ],
   "source": [
    "# generador de test data.\n",
    "test_generator = TensorSequence(test,BATCH_SIZE,embedder,MAX_CONTEXT,MAX_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "j = 0\n",
    "\n",
    "# Predecimos por batch\n",
    "for k in range(len(test_generator)):\n",
    "    print(k,len(test_generator))\n",
    "    batch = test_generator[k]\n",
    "    for i in range(len(batch[1][0])):\n",
    "        # agregamos la tupla real de inicio y fin.\n",
    "        y_true.append((np.argmax(batch[1][0][i]),np.argmax(batch[1][1][i])))\n",
    "    predict = model.predict_on_batch(batch[0])\n",
    "    for i in range(len(predict[1])):\n",
    "        #agregamos la tupla predecida.\n",
    "        y_pred.append((np.argmax(predict[0][i]),np.argmax(predict[1][i])))\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Mapeamos a arreglos 1 dimensionales para insertar en la función de sklearn.\n",
    "\n",
    "y_true_start = list(map(lambda x: x[0], y_true))\n",
    "y_pred_start = list(map(lambda x: x[0], y_pred))\n",
    "\n",
    "y_true_end = list(map(lambda x: x[1], y_true))\n",
    "y_pred_end = list(map(lambda x: x[1], y_pred))\n",
    "\n",
    "y_true_1d = list(map(lambda x: \"{0} {1}\".format(*x), y_true))\n",
    "y_pred_1d = list(map(lambda x: \"{0} {1}\".format(*x), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score inicio respuesta: 2.9356060606060606\n",
      "f1 score fin respuesta: 1.3636363636363635\n",
      "f1 score concatenacion: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 score inicio respuesta: {}\".format(f1_score(y_true_start, y_pred_start, average='micro')*100))\n",
    "print(\"f1 score fin respuesta: {}\".format(f1_score(y_true_end, y_pred_end, average='micro')*100))\n",
    "print(\"f1 score concatenacion: {}\".format(f1_score(y_true_1d, y_pred_1d, average='micro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
